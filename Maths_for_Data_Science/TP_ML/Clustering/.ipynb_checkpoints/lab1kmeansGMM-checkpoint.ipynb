{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $k$-means vs GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this lab is to compare the performance of two clustering algorithms, $k$-means and the EM algorithm for the Gaussian Mixture Model (GMM), on both synthetic and real data.\n",
    "\n",
    "You will find below Python code for loading data samples either from the Gaussian Mixture Model (GMM) or from the [iris dataset](https://fr.wikipedia.org/wiki/Iris_de_Fisher) and for clustering these data samples by $k$-means and EM on the GMM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compare the performance of $k$-means and Gaussian mixture. Test various parameters for the GMM, and different values of $k$.\n",
    "2. Implement a version of Gaussian mixture where the covariance matrices are diagonal (and thus stored as *vectors*) and test its performance. We refer to this algorithm as GMD (Gaussian Mixture with Diagonal covariance matrices)\n",
    "3. Test these three algorithms ($k$-means, GMM and GMD) on data of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_samples(mean = 0, std_dev = 1, nb_samples = 1):\n",
    "    '''Gaussian samples\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mean: float or array of floats, default: 0\n",
    "        Mean            \n",
    "    std_dev: float or array of floats, default: 1\n",
    "        Standard deviation (covariance = SS^T)\n",
    "    nb_samples: int\n",
    "        Number of samples\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    samples: array of floats of shape (nb_samples,dim)\n",
    "        Gaussian samples (std_dev * normal_samples + mean)\n",
    "    '''\n",
    "        \n",
    "    mean = np.array(mean)\n",
    "    std_dev = np.array(std_dev)\n",
    "    try:\n",
    "        dim = mean.shape[0]\n",
    "    except:\n",
    "        dim = 1\n",
    "    normal_samples = np.random.normal(size = dim * nb_samples).reshape(dim,nb_samples)\n",
    "    samples = np.array(std_dev).dot(normal_samples) \n",
    "    samples += np.array(mean).reshape(dim,1).dot(np.ones((1,nb_samples)))\n",
    "    return samples.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gmm_samples(means = [0], std_devs = [1], p = None, nb_samples = 1):\n",
    "    '''Gaussian mixture model samples\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    means: array of floats of shape (nb_modes,dim)\n",
    "        Means            \n",
    "    std_dev: array of floats of shape (nb_modes,dim,dim)\n",
    "        Standard deviations\n",
    "    p: array of floats of shape (nb_modes)\n",
    "        Mixing distribution (sums to 1)\n",
    "    nb_samples: int\n",
    "        Number of samples\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    samples: array of floats of shape (nb_samples,dim)\n",
    "        Gaussian mixture model samples \n",
    "    labels: array of integers of shape (nb_samples)\n",
    "        labels (latent variables)\n",
    "    '''\n",
    "    means = np.array(means)\n",
    "    std_devs = np.array(std_devs)\n",
    "    nb_labels = means.shape[0]\n",
    "    try:\n",
    "        dim = means.shape[1]\n",
    "    except:\n",
    "        dim = 1\n",
    "    labels = np.random.choice(nb_labels, size = nb_samples, p = p)\n",
    "    samples = np.zeros((nb_samples,dim))\n",
    "    for j in range(nb_labels):\n",
    "        nb_samples_j = np.sum(labels == j)\n",
    "        if nb_samples_j:\n",
    "            index = np.where(labels == j)[0]\n",
    "            samples[index] = gaussian_samples(means[j], std_devs[j], nb_samples_j)\n",
    "    return samples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(samples, labels, features = [0,1], feature_names = None, display_labels = True):\n",
    "    '''Display the samples in 2D'''\n",
    "    if display_labels:\n",
    "        nb_labels = np.max(labels)\n",
    "        for j in range(nb_labels + 1):\n",
    "            nb_samples = np.sum(labels == j)\n",
    "            if nb_samples:\n",
    "                index = np.where(labels == j)[0]\n",
    "                plt.scatter(samples[index,features[0]],samples[index,features[1]])\n",
    "    else:\n",
    "        plt.scatter(samples[:,features[0]],samples[:,features[1]],color='gray')\n",
    "    if feature_names is not None:\n",
    "        plt.xlabel(feature_names[0])\n",
    "        plt.ylabel(feature_names[1])\n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [[3,2],[-3,2],[0,-4]]\n",
    "std_devs = [[[1,0],[0,1]],[[1,0],[0,1]],[[2,0],[0,2]]]\n",
    "p = [0.25,0.25,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, labels = gmm_samples(means, std_devs, p, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_samples(samples, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "samples = iris.data  \n",
    "feature_names = iris.feature_names\n",
    "true_labels = iris.target\n",
    "label_names = list(iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [0,1]\n",
    "names = [feature_names[i] for i in features]\n",
    "show_samples(samples, true_labels, features, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [2,3]\n",
    "names = [feature_names[i] for i in features]\n",
    "show_samples(samples, true_labels, features, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score as ari\n",
    "from sklearn.metrics import adjusted_mutual_info_score as ami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    '''k-means algorithm\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_clusters: int, default: 8\n",
    "        Number of clusters.\n",
    "    \n",
    "    n_init : int, default: 10\n",
    "        Number of instances of k-means, each with different initial centers. \n",
    "        The output is that of the best instance (in terms of likelihood).\n",
    "    \n",
    "    n_iter: int, default: 300\n",
    "        Number of iterations for each instance of k-means.\n",
    "        \n",
    "    algorithm: \"random\" or \"++\", default:\"++\"\n",
    "        Algorithm for initializing the centers; \"++\" corresponds to k-means++.\n",
    "    \n",
    "    seed: int, default: None\n",
    "        Seed for the random generation of initial centers.\n",
    "        \n",
    "    verbose: boolean, optional\n",
    "        Verbose mode.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    labels_: array, shape(n_samples,)\n",
    "        Label of each sample (cluster index).\n",
    "        \n",
    "    centers_ : array, shape(n_clusters, n_features)\n",
    "        Cluster centers.\n",
    "        \n",
    "    inertias_: array, shape(n_clusters,)\n",
    "        Cluster inertias (sum of square distances).\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_clusters=8, n_init=10, n_iter=300, algorithm='++', seed=None, verbose = False):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_init = n_init\n",
    "        self.n_iter = n_iter\n",
    "        self.algorithm = algorithm\n",
    "        self.seed = seed\n",
    "        self.verbose = verbose\n",
    "        self.labels_ = None\n",
    "        self.centers_ = None\n",
    "        self.inertias_ = None\n",
    "       \n",
    "    def fit(self, X):\n",
    "        '''Cluster data X using k-means\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array, shape(n_samples,n_features)\n",
    "            Data to cluster.\n",
    "        '''        \n",
    "        \n",
    "        def init_centers(self, X):\n",
    "            if self.algorithm == 'random':\n",
    "                # random centers \n",
    "                samples = np.random.choice(X.shape[0], size = self.n_clusters)\n",
    "                centers = X[samples]\n",
    "            else:\n",
    "                # k-means++\n",
    "                centers = []\n",
    "                centers.append(X[np.random.randint(X.shape[0])])\n",
    "                distance = np.full(X.shape[0], np.inf)\n",
    "                for j in range(1,self.n_clusters):\n",
    "                    distance = np.minimum(np.linalg.norm(X - centers[-1], axis=1), distance)\n",
    "                    p = np.square(distance) / np.sum(np.square(distance))\n",
    "                    sample = np.random.choice(X.shape[0], p = p)\n",
    "                    centers.append(X[sample])\n",
    "            return centers\n",
    "        \n",
    "        def compute_centers(self, X, labels):\n",
    "            centers = []\n",
    "            for j in range(self.n_clusters):\n",
    "                index = np.where(labels == j)[0]\n",
    "                if len(index):\n",
    "                    centers.append(np.mean(X[index],axis = 0))\n",
    "                else:\n",
    "                    # reinit center in case of empty cluster\n",
    "                    centers.append(X[np.random.choice(X.shape[0])])\n",
    "            return np.array(centers)\n",
    "\n",
    "        def compute_distances(self, X, centers):\n",
    "            distances = []\n",
    "            for j in range(self.n_clusters):\n",
    "                distances.append(np.linalg.norm(X - centers[j], axis=1))\n",
    "            return np.array(distances)\n",
    "            \n",
    "        def compute_inertias(self, X, labels, centers):\n",
    "            inertias = []\n",
    "            for j in range(self.n_clusters):\n",
    "                index = np.where(labels == j)[0]\n",
    "                inertias.append(np.sum(np.square(np.linalg.norm(X[index] - centers[j], axis=1))))\n",
    "            return np.array(inertias)\n",
    "    \n",
    "        def one_kmeans(self, X):\n",
    "            centers = init_centers(self, X)\n",
    "            for i in range(self.n_iter):\n",
    "                centers_old = centers.copy()\n",
    "                distances = compute_distances(self, X, centers)\n",
    "                labels = np.argmin(distances, axis=0)  \n",
    "                centers = compute_centers(self, X, labels)\n",
    "                if np.array_equal(centers, centers_old):\n",
    "                    break\n",
    "            inertias = compute_inertias(self, X, centers, labels)\n",
    "            return labels, centers, inertias\n",
    "            \n",
    "        np.random.seed(self.seed)\n",
    "        best_inertia = None\n",
    "        # select the best instance of k-means\n",
    "        for i in range(self.n_init):\n",
    "            if self.verbose:\n",
    "                print(\"Instance \",i)\n",
    "            labels, centers, inertias = one_kmeans(self, X)\n",
    "            inertia = np.sum(inertias)\n",
    "            if best_inertia is None or inertia < best_inertia:\n",
    "                best_labels = labels.copy()\n",
    "                best_centers = centers.copy()\n",
    "                best_inertias = inertias.copy()\n",
    "                best_inertia = inertia\n",
    "\n",
    "        self.labels_ = best_labels\n",
    "        self.centers_ = best_centers\n",
    "        self.inertias_ = best_inertias\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.fit(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM:\n",
    "    '''EM algorithm for the Gaussian mixture model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_clusters: int, default: 8\n",
    "        Number of clusters.\n",
    "    \n",
    "    n_init : int, default: 10\n",
    "        Number of instances of the algorithm, each with different initial cluster centers. \n",
    "        The output is that of the best instance (in terms of likelihood).\n",
    "    \n",
    "    n_iter: int, default: 300\n",
    "        Maximum number of iterations for each instance of the algorithm.\n",
    "        \n",
    "    algorithm: \"random\" or \"k-means++\", default:\"k-means++\"\n",
    "        Algorithm for initializing the means.\n",
    "    \n",
    "    seed: int, default: None\n",
    "        Seed for the random generation of cluster centers.\n",
    "        \n",
    "    verbose: boolean, default: True\n",
    "        Verbose mode.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    labels_: array, shape(n_samples,)\n",
    "        Label of each sample (cluster index).\n",
    "\n",
    "    label_probs_: array, shape(n_samples,n_clusters)\n",
    "        Probability distribution of labels for each sample.\n",
    "        \n",
    "    centers_ : array, shape(n_clusters,n_features)\n",
    "        Cluster centers.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_clusters=8, n_init=10, n_iter=300, algorithm='k-means++', seed=None, verbose=True):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_init = n_init\n",
    "        self.n_iter = n_iter\n",
    "        self.algorithm = algorithm\n",
    "        self.seed = seed\n",
    "        self.verbose = verbose\n",
    "        self.labels_ = None\n",
    "        self.label_probs_ = None\n",
    "        self.centers_ = None\n",
    "       \n",
    "    def fit(self, X):\n",
    "        '''Cluster data X using EM for the Gaussian mixture model\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array, shape(n_samples,n_features)\n",
    "            Data to cluster.\n",
    "        '''        \n",
    "        \n",
    "        def init_parameters(self, X):\n",
    "            # choose initial centers\n",
    "            if self.algorithm == 'random':\n",
    "                # random centers\n",
    "                samples = np.random.choice(X.shape[0], size = self.n_clusters)\n",
    "                centers = X[samples]\n",
    "            else:\n",
    "                # k-means++\n",
    "                centers = []\n",
    "                centers.append(X[np.random.randint(X.shape[0])])\n",
    "                distance = np.full(X.shape[0], np.inf)\n",
    "                for j in range(1,self.n_clusters):\n",
    "                    distance = np.minimum(np.linalg.norm(X - centers[-1], axis=1), distance)\n",
    "                    p = np.square(distance) / np.sum(np.square(distance))\n",
    "                    sample = np.random.choice(X.shape[0], p = p)\n",
    "                    centers.append(X[sample])\n",
    "                \n",
    "            # estimate the parameters from the induced clusters\n",
    "            distances = []\n",
    "            for j in range(self.n_clusters):\n",
    "                distances.append(np.linalg.norm(X - centers[j], axis=1))\n",
    "            labels = np.argmin(np.array(distances), axis=0)\n",
    "            means = []\n",
    "            covariances = []\n",
    "            mixing_weights = []\n",
    "            for j in range(self.n_clusters):\n",
    "                index = np.where(labels == j)[0]\n",
    "                mixing_weights.append(len(index))\n",
    "                if len(index):\n",
    "                    means.append(np.mean(X[index], axis = 0))\n",
    "                    covariances.append(np.cov(X[index].T))    \n",
    "                else:\n",
    "                    means.append(centers[j])\n",
    "                    covariances.append(np.eye(X.shape[1]))\n",
    "            return np.array(means), np.array(covariances), np.array(mixing_weights)\n",
    "\n",
    "        def compute_label_probs(self, X, means, covariances, mixing_weights):\n",
    "            label_probs = np.zeros((X.shape[0],self.n_clusters))\n",
    "            for j in range(self.n_clusters):\n",
    "                cov = covariances[j]\n",
    "                try:\n",
    "                    inv_cov = np.linalg.inv(cov)\n",
    "                    square_distances = ((X - means[j]).dot(inv_cov) * (X - means[j])).sum(axis = 1) \n",
    "                    label_probs[:,j] = np.exp(-square_distances / 2) / np.sqrt(np.linalg.det(cov))\n",
    "                except:\n",
    "                    if self.verbose:\n",
    "                        print(\"Warning: Singular covariance matrix\")\n",
    "                    square_distances = np.square(X - means[j]).sum(axis = 1) \n",
    "                    label_probs[:,j] = np.exp(-square_distances / 2) \n",
    "            label_probs = label_probs * mixing_weights\n",
    "            label_probs = (label_probs.T / label_probs.sum(axis = 1)).T\n",
    "            return label_probs    \n",
    "                        \n",
    "        def compute_parameters(self, X, label_probs):\n",
    "            mixing_weights = label_probs.sum(axis = 0)\n",
    "            means = (X.T.dot(label_probs) / mixing_weights).T\n",
    "            covariances = []\n",
    "            for j in range(self.n_clusters):\n",
    "                Y = (X - means[j]).T * label_probs[:,j]\n",
    "                covariances.append(Y.dot(X - means[j]) / mixing_weights[j])\n",
    "            return means, covariances, mixing_weights\n",
    "        \n",
    "        def compute_log_likelihood(self, X, means, covariances, mixing_weights):\n",
    "            likelihoods = np.zeros((X.shape[0],self.n_clusters))\n",
    "            total_weight = mixing_weights.sum()\n",
    "            for j in range(self.n_clusters):\n",
    "                cov = covariances[j]\n",
    "                try:\n",
    "                    inv_cov = np.linalg.inv(cov)\n",
    "                    square_distances = ((X - means[j]).dot(inv_cov) * (X - means[j])).sum(axis = 1) \n",
    "                    likelihoods[:,j] = mixing_weights[j] / total_weight * np.exp(-square_distances / 2) / np.sqrt(np.linalg.det(cov))\n",
    "                except:\n",
    "                    if self.verbose:\n",
    "                        print(\"Warning: Singular covariance matrix\")\n",
    "                    square_distances = np.square(X - means[j]).sum(axis = 1) \n",
    "                    likelihoods[:,j] = mixing_weights[j] / total_weight * np.exp(-square_distances / 2) \n",
    "            return np.log(likelihoods.sum(axis = 1)).sum()\n",
    "    \n",
    "        def single_run_EM(self, X):\n",
    "            means, covariances, mixing_weights = init_parameters(self, X)\n",
    "            labels = -np.ones(X.shape[0])\n",
    "            for i in range(self.n_iter):    \n",
    "                # Expectation\n",
    "                label_probs = compute_label_probs(self, X, means, covariances, mixing_weights)              \n",
    "                if np.array_equal(labels, label_probs.argmax(axis = 1)):\n",
    "                    break\n",
    "                else:\n",
    "                # Maximization\n",
    "                    means, covariances, mixing_weights = compute_parameters(self, X, label_probs)\n",
    "                    labels = label_probs.argmax(axis = 1)\n",
    "            return label_probs, means, covariances, mixing_weights\n",
    "            \n",
    "        np.random.seed(self.seed)\n",
    "        best_loglikelihood = None\n",
    "        # select the best instance of EM\n",
    "        for i in range(self.n_init):\n",
    "            if self.verbose:\n",
    "                print(\"Instance \",i)               \n",
    "            label_probs, means, covariances, mixing_weights = single_run_EM(self, X)\n",
    "            loglikelihood = compute_log_likelihood(self, X, means, covariances, mixing_weights)\n",
    "            if best_loglikelihood is None or loglikelihood > best_loglikelihood:\n",
    "                best_loglikelihood = loglikelihood\n",
    "                best_label_probs = label_probs\n",
    "                best_labels = label_probs.argmax(axis = 1)\n",
    "                best_centers = means\n",
    "        self.label_probs_ = best_label_probs\n",
    "        self.labels_ = best_labels\n",
    "        self.centers_ = best_centers\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GMM(n_clusters = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm.fit(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian mixture with diagonal covariance matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMD:\n",
    "    '''EM algorithm for the Gaussian mixture model with diagonal covariance matrices\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_clusters: int, default: 8\n",
    "        Number of clusters.\n",
    "    \n",
    "    n_init : int, default: 10\n",
    "        Number of instances of the algorithm, each with different initial cluster centers. \n",
    "        The output is that of the best instance (in terms of likelihood).\n",
    "    \n",
    "    n_iter: int, default: 300\n",
    "        Maximum number of iterations for each instance of the algorithm.\n",
    "        \n",
    "    algorithm: \"random\" or \"k-means++\", default:\"k-means++\"\n",
    "        Algorithm for initializing the means.\n",
    "    \n",
    "    seed: int, default: None\n",
    "        Seed for the random generation of cluster centers.\n",
    "        \n",
    "    verbose: boolean, default: True\n",
    "        Verbose mode.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    labels_: array, shape(n_samples,)\n",
    "        Label of each sample (cluster index).\n",
    "\n",
    "    label_probs_: array, shape(n_samples,n_clusters)\n",
    "        Probability distribution of labels for each sample.\n",
    "        \n",
    "    centers_ : array, shape(n_clusters,n_features)\n",
    "        Cluster centers.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_clusters=8, n_init=10, n_iter=300, algorithm='k-means++', seed=None, verbose=True):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_init = n_init\n",
    "        self.n_iter = n_iter\n",
    "        self.algorithm = algorithm\n",
    "        self.seed = seed\n",
    "        self.verbose = verbose\n",
    "        self.labels_ = None\n",
    "        self.label_probs_ = None\n",
    "        self.centers_ = None\n",
    "       \n",
    "    def fit(self, X):\n",
    "        '''Cluster data X using EM for the Gaussian mixture model with diagonal covariance matrices\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array, shape(n_samples,n_features)\n",
    "            Data to cluster.\n",
    "        '''        \n",
    "        \n",
    "        # to be completed\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmd = GMD(n_clusters = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmd.fit(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
