{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PacMan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates the concepts and algorithms of reinforcement learning to collect rewards in a maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_maze, display_policy, display_value, display_diff_policy, normalize_sparse, get_moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = sparse.load_npz('pacman.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_maze(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_target = [(13,1), (11,11)]\n",
    "states_restart = [[(7,5), (3,7)], [(7,9), (5,3)]]\n",
    "rewards_target = [100, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    ----------\n",
    "    maze: sparse csr matrix\n",
    "        Binary map of the maze\n",
    "    states: list of tuples\n",
    "        States\n",
    "    states_target: list of tuples\n",
    "        Target states (with rewards)\n",
    "    states_restart: list of list of tuples\n",
    "        List of restart states (taken uniformly at random)\n",
    "    rewards_target: list of float\n",
    "        Rewards of target states\n",
    "    state_id: dict\n",
    "        Id of each state\n",
    "    targets: numpy array\n",
    "        Ids of target states        \n",
    "    restarts: list of numpy array\n",
    "        Ids of restart states\n",
    "    adjacency: sparse csr matrix\n",
    "        Matrix of adjacent ids \n",
    "    rewards: numpy array\n",
    "        Reward of each state\n",
    "    gamma: float (default = 1)\n",
    "        Discount factor\n",
    "    \"\"\"\n",
    "    def __init__(self, maze, states_target = None, states_restart = None, rewards_target = None, gamma = 1):\n",
    "        n = maze.nnz\n",
    "        maze_coo = sparse.coo_matrix(maze)\n",
    "        states = [(maze_coo.row[i], maze_coo.col[i]) for i in range(n)]\n",
    "\n",
    "        if states_target is None:\n",
    "            states_target = [np.random.choice(states)]\n",
    "        if states_restart is None:\n",
    "            states_restart = [[np.random.choice(list(set(states) - set(s)), size = 2)] for s in states_target]\n",
    "        if rewards_target is None:\n",
    "            rewards_target = [np.random.choice(100) for s in states_target]\n",
    "\n",
    "        state_id = {s:i for i, s in enumerate(states)}\n",
    "\n",
    "        targets = np.array([state_id[s] for s in states_target])\n",
    "        restarts = []\n",
    "        for states_ in states_restart:\n",
    "            restarts.append(np.array([state_id[s] for s in states_]))\n",
    "\n",
    "        row = []\n",
    "        col = []\n",
    "        for (i,j) in state_id:\n",
    "            if (i + 1, j) in state_id:\n",
    "                row.append(state_id[(i, j)])\n",
    "                col.append(state_id[(i + 1, j)])\n",
    "            if (i, j + 1) in state_id:\n",
    "                row.append(state_id[(i, j)])\n",
    "                col.append(state_id[(i, j + 1)])\n",
    "        adjacency = sparse.csr_matrix((np.ones_like(row), (row, col)), shape = (n, n))   \n",
    "        \n",
    "        rewards = -np.ones(n)\n",
    "        rewards[targets] += rewards_target\n",
    "        \n",
    "        self.maze = maze\n",
    "        self.states = states\n",
    "        self.states_target = states_target\n",
    "        self.states_restart = states_restart\n",
    "        self.rewards_target = rewards_target\n",
    "        self.state_id = state_id\n",
    "        self.targets = targets\n",
    "        self.restarts = restarts\n",
    "        self.adjacency = adjacency + adjacency.T\n",
    "        self.rewards = rewards\n",
    "        self.gamma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(maze, states_target, states_restart, rewards_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_maze(model.maze, states_target = model.states_target, states_restart = model.states_restart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A policy is defined as a sparse transition matrix between states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition(policy, model):\n",
    "    transition = sparse.lil_matrix(policy)\n",
    "    # no action in target states\n",
    "    targets = model.targets\n",
    "    restarts = model.restarts\n",
    "    for k in range(len(targets)):\n",
    "        transition[targets[k]] = 0\n",
    "        transition[targets[k], restarts[k]] = 1\n",
    "    transition = normalize_sparse(sparse.csr_matrix(transition))\n",
    "    return transition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random policy\n",
    "policy = normalize_sparse(model.adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, V = None, model = model, n_iter_eval = 1000):\n",
    "    \"\"\"Evaluate a policy by iterations, starting from V\"\"\"\n",
    "    rewards = model.rewards\n",
    "    gamma = model.gamma\n",
    "    transition = get_transition(policy, model)\n",
    "    if V is None:\n",
    "        V = np.zeros_like(rewards)\n",
    "    for t in range(n_iter_eval):\n",
    "        V = transition.dot(rewards + gamma * V)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_from_value(V, model = model):\n",
    "    \"\"\"Get the greedy policy associated with V\"\"\"\n",
    "    n = len(model.states)\n",
    "    rewards = model.rewards\n",
    "    gamma = model.gamma\n",
    "    row = []\n",
    "    col = []\n",
    "    for i in range(n):\n",
    "        indices = model.adjacency[i].indices\n",
    "        values = (rewards + gamma * V)[indices]\n",
    "        j = indices[np.argmax(values)]\n",
    "        row.append(i)\n",
    "        col.append(j)\n",
    "    policy = normalize_sparse(sparse.csr_matrix((np.ones_like(row), (row, col)), shape = (n, n)))\n",
    "    return policy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_policy(policy, model = model, n_iter_max = 100, n_iter_eval = 1000, verbose = True, display = False):\n",
    "    \"\"\"Get the optimal policy by policy iteration\"\"\"\n",
    "    V = None\n",
    "    policy_prev = sparse.csr_matrix(policy.shape)\n",
    "    t = 0\n",
    "    while (policy - policy_prev).nnz and t < n_iter_max:\n",
    "        if display:\n",
    "            display_diff_policy(model.maze, get_moves(policy, model), get_moves(policy_prev, model))\n",
    "        V = evaluate_policy(policy, V, model, n_iter_eval)\n",
    "        policy_prev = policy.copy()\n",
    "        policy = get_policy_from_value(V, model)\n",
    "        t += 1\n",
    "    if verbose:\n",
    "        print(\"Number of iterations =\", t - 1)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = iterate_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_value(model.maze, V = evaluate_policy(policy), states_target = model.states_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "moves = get_moves(policy, model)\n",
    "display_policy(model.maze, moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial values\n",
    "V = np.zeros(len(model.states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_value(V, model = model, n_iter = 1000, verbose = True):\n",
    "    \"\"\"Get the optimal policy by value iteration\"\"\"\n",
    "    policy = get_policy_from_value(V, model)\n",
    "    for t in range(n_iter):\n",
    "        V = evaluate_policy(policy, V, model, 1)\n",
    "        policy = get_policy_from_value(V, model)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = iterate_value(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_value(model.maze, V = evaluate_policy(policy), states_target = model.states_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moves = get_moves(policy, model)\n",
    "display_policy(model.maze, moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial values of Q\n",
    "policy = normalize_sparse(model.adjacency)\n",
    "Q = get_transition(policy, model)\n",
    "Q.data = np.zeros_like(Q.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action is identified with new state (after move) except for restarts (no action)\n",
    "\n",
    "def sarsa(Q, model = model, alpha = 0.1, eps = 0.1, n_iter = 100):\n",
    "    targets = model.targets\n",
    "    rewards = model.rewards\n",
    "    gamma = model.gamma\n",
    "    # random state (not in targets)\n",
    "    state = np.random.choice(np.setdiff1d(np.arange(len(model.states)), targets))\n",
    "    # random action\n",
    "    action = np.random.choice(Q[state].indices)\n",
    "    new_state = action\n",
    "    for t in range(n_iter):\n",
    "        state_prev = state\n",
    "        action_prev = action\n",
    "        state = new_state\n",
    "        if state in targets:\n",
    "            action = np.min(Q[state].indices)\n",
    "            new_state = np.random.choice(Q[state].indices)\n",
    "        else:\n",
    "            best_action = Q[state].indices[np.argmax(Q[state].data)]\n",
    "            if np.random.random() < eps:\n",
    "                action = np.random.choice(Q[state].indices)\n",
    "            else:\n",
    "                action = best_action\n",
    "            new_state = action\n",
    "        Q[state_prev, action_prev] = (1 - alpha) *  Q[state_prev, action_prev] + alpha * (rewards[action_prev] + gamma * Q[state, action])\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_from_action_value(Q, model = model):\n",
    "    \"\"\"Get the greedy policy associated with Q\"\"\"\n",
    "    n = len(model.states)\n",
    "    row = []\n",
    "    col = []\n",
    "    for i in range(n):\n",
    "        indices = Q[i].indices\n",
    "        j = indices[np.argmax(Q[i].data)]\n",
    "        row.append(i)\n",
    "        col.append(j)\n",
    "    policy = normalize_sparse(sparse.csr_matrix((np.ones_like(row), (row, col)), shape = (n, n)))\n",
    "    return policy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = sarsa(Q, n_iter = n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = get_policy_from_action_value(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "moves = get_moves(policy, model)\n",
    "display_policy(model.maze, moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
