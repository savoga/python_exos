{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06/11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le perceptron monocouche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut classer un input $X$ (plusieurs vecteurs de réels) de façon binaire; <br>\n",
    "=> on cherche à approcher $f: \\mathcal{X} \\to \\mathcal{Y}$ où $X \\in \\mathcal{X} \\subset \\mathbb{R}^p$ et $\\mathcal{Y}=\\{-1,1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperplan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le perceptron monocouche, la classification se fait de manière <i>linéaire</i>; on cherche un <i>hyperplan</i> qui sépare au mieux les observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathcal{H}=\\{ x \\in \\mathbb{R}^p, \\hat{f}_\\omega(x):=\\omega_0+\\Sigma_{i=1}^p \\omega_ix_i=0 \\}$$<br>\n",
    "<center>($x$ est l'ensemble des variables explicatives pour <u>une</u> observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi écrire:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathcal{H}_\\omega:\\omega^Tx+\\omega_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color ='red'>Classifieur Perceptron: $x \\mapsto sign(\\hat{f}_\\omega(x))$ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ERM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les $\\omega$ doivent être tel que la fonction de perte $\\ell$ (risque) est minimisée:<br>\n",
    "$\\mathbb{E}[\\mathcal{\\ell}(\\hat{f}_\\omega(x),y)]$ est le risque <i>théorique</i> à minimiser<br>\n",
    "En machine learning on respecte la théorie ERM (Empiric Risk Minimisation) et donc se concentre sur le risque empirique<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fonction de perte (= risque empirique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\widehat{L}_n(f_\\omega(x))=\\frac{1}{n}\\Sigma \\mathbb{1} \\{ -Y_i (\\omega_0+\\Sigma_{i=1}^p \\omega_ix_i) >0 \\}$ --> Les résultats pris en compte sont lorsque le signe de $Y_i$ est différent du signe de $\\omega_0+\\Sigma_{i=1}^p \\omega_ix_i$<br>\n",
    "Problème: la fonction de perte $\\widehat{L}_n$ n'est pas continue (car présence d'une indicatrice), donc on ne peut pas appliquer la descente de gradient.<br>\n",
    "=> Rosenblatt utilise une fonction de perte lisse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi prendre: $\\widehat{L}_n(f_\\omega(x))=-\\Sigma Y_i (\\omega_0+\\Sigma_{i=1}^p \\omega_ix_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut minimiser la fonction de perte $\\widehat{L}_n$ --> implique une somme (espérance => somme dans la grandeur empirique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Minimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rappel: descente de gradient globale:<br>\n",
    "$\\widehat{L}_n(f_\\omega(x))=\\Sigma_{i=1}^n \\mathcal{\\ell}(\\hat{f}_\\omega(x),y)$<br>\n",
    "$E=1000$<br>\n",
    "$\\epsilon$ = petite valeur<br>\n",
    "$\\omega_0$ = valeur initiale en $t_0$<br>\n",
    "$tant~que~(E>\\epsilon):$<br>\n",
    "$~~~~\\omega_{t+1}=\\omega_t-\\epsilon\\Sigma_{i=1}^n\\nabla_\\omega{\\mathcal{\\ell}(\\hat{f}_\\omega(x),y)}$<br>\n",
    "$~~~~calculer~E=L_n(\\omega_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descente de gradient stochastique:<br>\n",
    "$\\widehat{L}_n(f_\\omega(x))=\\Sigma_{i=1}^n \\mathcal{\\ell}(\\hat{f}_\\omega(x),y)$<br>\n",
    "$E=1000$<br>\n",
    "$\\epsilon$ = petite valeur<br>\n",
    "$\\omega_0$ = valeur initiale en $t_0$<br>\n",
    "$tant~que~(E>\\epsilon):$<br>\n",
    "$~~~~for~i=1~to~n:$<br>\n",
    "$~~~~~~~~~\\omega_{t+1}=\\omega_t-\\epsilon\\nabla_\\omega{\\mathcal{\\ell}(\\hat{f}_\\omega(x_i),y_i)}$<br>\n",
    "$~~~~~~~~~calculer~E=L_n(\\theta_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descente de gradient stochastique aléatoire:<br>\n",
    "$\\widehat{L}_n(f_\\omega(x))=\\Sigma_{i=1}^n \\mathcal{\\ell}(\\hat{f}_\\omega(x),y)$<br>\n",
    "$E=1000$<br>\n",
    "$\\epsilon$ = petite valeur<br>\n",
    "$\\omega_0$ = valeur initiale en $t_0$<br>\n",
    "$tant~que~(E>\\epsilon):$<br>\n",
    "$~~~~for~i=1~to~n:$<br>\n",
    "$~~~~~~~~~tirer~uniformément~i \\in \\{1,...,n\\}$<br>\n",
    "$~~~~~~~~~\\omega_{t+1}=\\omega_t-\\epsilon\\nabla_\\omega{\\mathcal{\\ell}(\\hat{f}_\\omega(x_i),y_i)}$<br>\n",
    "$~~~~~~~~~calculer~E=L_n(\\theta_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: le tirage peut se faire avec ou sans remise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
