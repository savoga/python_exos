{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06/11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rappel de notations: \n",
    "\n",
    "$x=(x_1,...,x_p)^T \\in \\mathcal{X}$ une observation<br>\n",
    "$\\mathcal{D}_n=\\{(x_i,y_i),i=1,...,n\\}$ ensemble d’apprentissage contenant les n exemples et leurs étiquettes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut approcher $f: \\mathcal{X} \\to \\mathcal{Y}$, $\\mathcal{X} \\in \\mathbb{R}^p$, $\\mathcal{Y}=\\{1,...,L\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque nouveau point $x \\in \\mathbb{R}^p$ on détermine l’ensemble de ses k-plus proches voisins parmi tous les points d’apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On note la distance entre 2 points: $d: \\mathbb{R}^p \\times \\mathbb{R}^p \\to \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Attention: un \"point\" $i$ est une <u>observation</u> $x^T=(x_i^{(1)},...,x_i^{(p)})$; ici on calcule donc des distances entre des vecteurs de dimension $p$</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut prendre la distance euclidienne: $d(u,v)=||u-v||^2=\\Sigma_{i=1}^p(u_i-v_i)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a donc une matrice de distances avec en colonne l'échantillon de test (exposant $(t)$) et en ligne l'échantillon d'entraînement (exposant $(T)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_style(val):\n",
    "              return 'font-weight: bold'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_1^{(t)}$ | $x_2^{(t)}$ | $...$ | $x_m^{(t)}$\n",
    "------ | ----- | ----- | -----\n",
    "$d(x_1^{(t)},x_1^{(T)})$ | $d(x_2^{(t)},x_1^{(T)})$ | $...$ | $d(x_m^{(t)},x_1^{(T)})$\n",
    "$d(x_1^{(t)},x_2^{(T)})$ | $d(x_2^{(t)},x_1^{(T)})$ | $...$ | $...$\n",
    "$...$ | $...$ | $...$ | $...$\n",
    "$d(x_1^{(t)},x_n^{(T)})$ | $...$ | $...$ | $d(x_m^{(t)},x_n^{(T)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: pour cette matrice, l'échantillon d'apprentissage est de taille $n$, celui de test de taille $m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On trie ensuite cette matrice sur les <b>colonnes</b> par ordre croissant pour trouver les points d'entraînement dont les distances avec le point testé sont les plus faibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque point testé, on garde les k plus petites distances et on regarde les classes associées à chacun des points sélectionnés (la fonction <i>argsort</i> permet de garder les indices au moment du tri). On définit le rang d'un voisin:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$r_k(x)=i^*$ si et seulement si $d(x_{i^*},x)=\\underset{1 \\le i \\le n \\\\\n",
    "i \\neq r_{1},...,r_{k-1}}{\\operatorname{min}}d(x_i,x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La règle la plus basique est de prendre la classe majoritaire parmi ces k voisins sélectionnés (schéma ci-dessous):\n",
    "\n",
    "$$\\widehat{f}_k(x)=\\underset{y \\in \\mathcal{Y}}{\\operatorname{argmax}}(\\Sigma_{j=1}^k \\mathbb{1}\\{y_{r_j}=y\\})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"KNN_pic.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque: la fonction <i>fit</i> du KNNClassifier est juste une affectation des données d'entraînement à l'objet (le calcul des distances se fera dans le <i>predict</i>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\" Homemade kNN classifier class \"\"\"\n",
    "    def __init__(self, n_neighbors=1):\n",
    "        self.n_neighbors = n_neighbors\n",
    "    \n",
    "    def fit(self, X, y): # L'entraînement sert simplement à \"enregistrer\" les données d'entraînement,\n",
    "                         # déjà classifiées\n",
    "        self.X = X\n",
    "        self.Y = y\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        n = len(self.X) # taille de l'échantillon dans X_train\n",
    "        m = len(X) # taille de l'échantillon dans X_test\n",
    "        dist_mat = []\n",
    "        for i in range(n): # On boucle sur tous les éléments de l'échantillon d'entraînement\n",
    "            dist_vect = []\n",
    "            for j in range(m): # On boucle sur tous les éléments de l'échantillon de test\n",
    "                dist_vect.append(euclidean_distance(self.X[i], X[j]))\n",
    "            dist_mat.append(dist_vect) # len(dist_vect) = m (nb de features; toutes les distances pour une observation)\n",
    "            \n",
    "        dist_mat = np.asarray(dist_mat) # dist_mat.shape = (n,m); T_test en colonne, X_train en ligne\n",
    "        \n",
    "        # dist_mat = metrics.pairwise.pairwise_distances(X, Y=self.X, metric='euclidean', n_jobs=1)\n",
    "            \n",
    "        idx_sort = np.argsort(dist_mat, kind='mergesort', axis=0)  # idx_sort.shape = (n,m); dist_mat triée selon les colonnes\n",
    "        # mergesort donne une gestion stable des nombre égaux: \n",
    "        # en cas d'égalité, l'ordre des indices dans l'output est le même que l'ordre dans l'input\n",
    "        \n",
    "        idx_sort_knn = idx_sort[:self.n_neighbors,:] # Redimensionnement avec le nombre de plus proches voisins\n",
    "        return getBestClassFromCount(idx_sort_knn, self.Y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avantages:<br>\n",
    "- intuitif\n",
    "- aisément paramétrable\n",
    "- nombre de classes quelconques\n",
    "\n",
    "Inconvénients:<br>\n",
    "- très couteux (distances à calculer) => pas adapté pour le big data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\widehat{f}_k(x)=\\underset{y \\in \\mathcal{Y}}{\\operatorname{argmax}}(\\Sigma_{j=1}^k \\omega_j \\mathbb{1}\\{y_{r_j}=y\\})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "où $$\\omega_j = e^{-d_j^2/h}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention: cette variante ne change rien dans le sélection des k-ppv, c'est uniquement au moment de la sélection de la classe majoritaire qu'on affine la sélection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette variante donne plus de poids aux distances très petites. Plus $h$ est petit, plus les petites distances sont favorisées (fonction $exp$ plus pentue)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basé sur l'exemple de Google Developers: https://github.com/random-forests/tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cherche à approcher $f: \\mathcal{X} \\to \\mathcal{Y}$ où $\\mathcal{X} \\subset \\mathbb{R}^p$ et $\\mathcal{Y}$ l'ensemble des étiquettes. $\\mathcal{S}$ est l'ensemble d'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut parvenir à partitionner l'espace $\\mathcal{X}$ au mieux. Au départ $\\mathcal{X}$ est le noeud (racine)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme principal permettant de construire un arbre est le suivant (récursif):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(rows):\n",
    "    gain, question = find_best_split(rows)\n",
    "    if gain == 0:\n",
    "        return Leaf(rows)\n",
    "    true_rows, false_rows = partition(rows, question)\n",
    "    true_branch = build_tree(true_rows)\n",
    "    false_branch = build_tree(false_rows)\n",
    "    return Decision_Node(question, true_branch, false_branch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algo d'optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque ensemble de données, on définit le bon séparateur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(rows):\n",
    "    best_gain = 0\n",
    "    best_question = None\n",
    "    current_gini = gini(rows)\n",
    "    n_features = len(rows[0]) - 1\n",
    "\n",
    "    for col in range(n_features): # Itération sur les variables explicatives\n",
    "\n",
    "        values = set([row[col] for row in rows]) # Itération sur les observations d'une variable explicative\n",
    "\n",
    "        for val in values: # Itération sur les observations uniques\n",
    "\n",
    "            question = Question(col, val)\n",
    "\n",
    "            true_rows, false_rows = partition(rows, question)\n",
    "\n",
    "            if len(true_rows) == 0 or len(false_rows) == 0:\n",
    "                continue\n",
    "\n",
    "            gain = info_gain(true_rows, false_rows, current_gini)\n",
    "\n",
    "            if gain >= best_gain:\n",
    "                best_gain, best_question = gain, question\n",
    "\n",
    "    return best_gain, best_question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une <i>question</i> est donc une combinaison (variable explicative, valeur) à partir de laquelle on va regarder les variables à droite ($\\ge$) et à gauche ($\\le$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mesure de l'impureté: l'indice de Gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une méthode pour séparer les données est <i>l'indice de Gini</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(rows):\n",
    "    counts = class_counts(rows) # Renvoie un dictionnaire d'occurences\n",
    "    impurity = 1\n",
    "    for lbl in counts:\n",
    "        prob_of_lbl = counts[lbl] / float(len(rows))\n",
    "        impurity -= prob_of_lbl**2\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'indice de Gini permet de mesurer l'impureté de la séparation. Pour une liste d'éléments, l'indice de Gini représente la proba de mal classer un élément si on le tire au hasard et qu'on le classe au hasard. Autrement dit, la mesure de Gini représente la disparité au sein d'un ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H(S)=\\Sigma_{\\ell=1}^C p_{\\ell}(S)(1-p_{\\ell}(S))$ qu'on peut aussi écrire (plus facile à coder):<br> $H(S)=\\Sigma_{\\ell=1}^C p_{\\ell}(S)-\\Sigma_{\\ell=1}^C p_{\\ell}(S)*p_{\\ell}(S)=1-\\Sigma_{\\ell=1}^C p_{\\ell}(S)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avec $p_{c}(S)=\\frac{1}{n}\\Sigma_{i=1}^n \\mathbb{1}\\{y_i=c\\}$ la fréquence d'une étiquette dans un ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction de perte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour mesurer la pertinence du split, on utilise <i>information gain</i> qui compare l'impureté d'un état de l'arbre actuel (<i>current_gini</i>) avec l'impureté calculée si on applique un séparateur. Cette deuxième mesure de l'impureté est en fait la fonction de perte qui pondère la mesure de gini sur l'importance de ses enfants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(left, right, current_gini):\n",
    "    p = float(len(left)) / (len(left) + len(right))\n",
    "    return current_gini - (p * gini(left) + (1 - p) * gini(right))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parmi toutes les variables et leurs valeurs $(j, \\tau)$, on cherche $(\\widehat{j}, \\widehat{\\tau})$ qui minimisent:\n",
    "\n",
    "$\\mathcal{L}(t_{j,\\tau},S)=\\frac{n_d}{n}H(\\mathcal{D}(S,j,\\tau))+\\frac{n_g}{n}H(\\mathcal{G}(S,j,\\tau))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le problème s'écrit donc:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underset{j \\in \\{1,...,p\\} \\\\\n",
    "\\tau \\in \\mathbb{R}}{\\operatorname{argmin}} \\frac{n_d}{n}H(\\mathcal{D}(S,j,\\tau))+\\frac{n_g}{n}H(\\mathcal{G}(S,j,\\tau))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathcal{D}(S,j,\\tau)=\\{(x,y), t_{j,\\tau} \\ge 0\\}$ l'ensemble des éléments classés à droite => l'inégalité $t_{j,\\tau} \\ge 0$ représente le critère de classification (<i>séparateur linéaire</i>) gauche/droite. Les arbres utilisent donc plusieurs séparateurs linéaires pour construire des frontières de décision non linéaires. Utiliser des séparateurs linéaires orthogonaux à chaque vecteur de base (<font color='orange'>??</font>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n_d=\\frac{|i \\in [\\![ 1,n ]\\!]: x_i \\in D(j,\\tau) |}{\\{i' \\in [\\![ 1,n ]\\!]: x_{i'} \\in G(j,\\tau) \\cup D(j,\\tau)\\}}=|\\mathcal{D}(S,j,\\tau)|$ est le nombre d'éléments qui ont été classés dans le noeud droit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention: le problème global est la minimisation de la fonction de perte, ce qui revient à maximiser la différence d'impureté à chaque test de séparateur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le critère d'arrêt c'est lorsque qu'on ne peut pas diminuer la fonction de coût avec un séparateur. Alors, on dit qu'on est dans une feuille."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction de prédiction prend en paramètre la nouvelle observation, ainsi que l'arbre construit précédemment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(row, node):\n",
    "    if isinstance(node, Leaf):\n",
    "        return node.predictions\n",
    "\n",
    "    if node.question.match(row):\n",
    "        return classify(row, node.true_branch)\n",
    "    else:\n",
    "        return classify(row, node.false_branch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On parcourt l'arbre jusqu'à arriver à une feuille qui donne les probabilités de résultats associées:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaf:\n",
    "    def __init__(self, rows):\n",
    "        self.predictions = class_counts(rows) # renvoie un dictionnaire des classes avec leurs occurences (= proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les arbres de décision ont une variance très elevée car pour chaque observation $x_i$ les résultats $y_i$ sont très différents. Ainsi les écarts à la moyenne sont élevés:\n",
    "\n",
    "$$Var_n(y) = \\frac{1}{N}\\Sigma_{i=1}^N(y_i - \\overline{y})^2$$\n",
    "\n",
    "Intuitivement, une variance élevée représente la sensibilité de la fonction prédictive aux données d'entraînement. Ce problème réside dans la hierarchisation de la méthode: une légère modification dans un noeud est répercutée chez les noeuds enfants."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
