{\fontsize{12pt}{22pt} \textbf{Perceptron}\par}

\vspace{5mm}

The Perceptron is the most basic algorithm for binary classification. \\

We want to estimate $f: \mathcal{X} \to \mathcal{Y}$ where $X \in \mathcal{X} \subset \mathbb{R}^p$ and $\mathcal{Y}=\{-1,1\}$ \\

\underline{Hyperplane} \\

The classification is linear. We look for an hyperplane that separates the best the observations. \\

$$\mathcal{H}=\{ x \in \mathbb{R}^p, \hat{f}_\omega(x):=\omega_0+\Sigma_{i=1}^p \omega_ix_i=0 \}$$ \\

We can also write:

$$\mathcal{H}_\omega:\omega^Tx+\omega_0$$ \\

Perceptron classifier: $x \mapsto sign(\hat{f}_\omega(x))$ \\

\underline{Loss function} \\

We want to find $\omega$ such that the loss function $\ell$ is minimised:

$\mathbb{E}[\mathcal{\ell}(\hat{f}_\omega(x),y)]$ is the theoric risk to minimize.

Recall that ERM consists of minimizing the empiric risk. \\

\underline{Empiric risk} \\

The following is an example of loss function that can be used for the Perceptron problem:

$$\widehat{L}_n(\hat{f}_\omega(x))=-\Sigma Y_i (\omega_0+\Sigma_{i=1}^p \omega_ix_i)$$

(when $Y_i$ (target) and $\omega_0+\Sigma_{i=1}^p \omega_ix_i$ (predicted) have different signs, the loss function increases)

\vspace{5mm}