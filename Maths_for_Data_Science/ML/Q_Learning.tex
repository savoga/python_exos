\underline{Q-learning}

\vspace{5mm}

This algorithm is also based on $\epsilon$-greedy algorithm.

  \begin{equation}
 \pi(s) \leftarrow
    \begin{cases}
     a^* \in argmax_a Q(s,a) $ with probability $ 1-\epsilon \\
    random $ with probability $ \epsilon
    \end{cases}
  \end{equation}

\textbf{Estimation}: unlike SARSA, Q-learning aims at updating the estimator using the best action at each iteration:

\begin{center}
$\forall t, Q(s_t, a_t) \xleftarrow{\alpha} r_t + \gamma max_{a} Q(s_{t+1}, a)$
\end{center}

The only modification from SARSA is the following line:

\lstset{language=Python}
\lstset{frame=lines}
\lstset{caption={Q-Learning}}
\lstset{label={lst:code_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
Q[state_prev, action_prev] = (1 - alpha) *  Q[state_prev, action_prev] 
			+ alpha * (rewards[action_prev] + gamma * np.max(Q[state].data))
\end{lstlisting}

\vspace{5mm}