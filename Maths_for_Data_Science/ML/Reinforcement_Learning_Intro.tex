Reinforcement learning is inspired on human logic: we learn which strategy to take thanks to rewards we receive.

\vspace{5mm}

Reinforcement learning uses Markov Decision Processes (MDP).

A Markov Decision Process is defined by:

- an initial state $s_0$

- the reward distribution $r_t \sim p(r | s_t, a_t)$ (stochastic)

- the transition probabilities, $s_{t+1} \sim p(s | s_t, a_t)$ (stochastic)

=> \textbf{MDP: we know the state and the reward from the previous state only}.

\vspace{5mm}

A \textit{policy} is an action for each state, for a given MDP.

=> \textbf{a policy can be seen as a strategy: we know where to go at each state}. In practice the policy is actually the \textbf{transition probability matrix}.

\vspace{5mm}

The \textit{value function} is the gain we earn at a state, for a specific policy: $\forall s, V_\pi(s) = \mathbb{E}_{\pi}[G | s_0 = s]$

=> \textbf{The expected gain takes into account the transition probability.}

The gain is calculated as such: $G = r_0 + \gamma r_1 + \gamma^2 r_2 +... = \Sigma_t \gamma^t r_t $ where $\gamma$ is the \textit{discount factor}.

$V$ can also be written $V_\pi(s) = \mathbb{E}_\pi[r_0 + \gamma V(s_1) | s_0 = s]$. This expression is called \textbf{Bellman equation}.

\vspace{5mm}

We can find the best policy:

$\forall s, \pi^*(s) = a^* \in argmax_a\mathbb{E}_\pi[r_0 + \gamma V_*(s_1) | s_0 = s, a_0 = a]$

Where $V_*$ is solution of the \textbf{Bellman optimality equation}:

$\forall s, V(s) = max_a\mathbb{E}[r_0 + \gamma V(s_1) | s_0 = s]$

\vspace{5mm}