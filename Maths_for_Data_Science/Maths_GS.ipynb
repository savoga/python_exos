{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En <b>gras</b> les titres de parties ou les mots importants<br>\n",
    "En <font color='red'>rouge</font> les théorèmes ou définitions<br>\n",
    "En <font color='blue'>bleu</font> les rappels<br>\n",
    "En <font color='orange'>orange</font> les todo<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i>09/09</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rappels stats & proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Grandeurs empiriques VS grandeurs théoriques</u><br>\n",
    "<u>Modèle statistique</u><br>\n",
    "<u>Modèle statistique paramétrique</u><br>\n",
    "<u>Estimation</u><br>\n",
    "<u>Risque quadratique</u><br>\n",
    "<u>Statistiques exploratoires et descriptives</u><br>\n",
    "<u>Variance / écart type</u><br>\n",
    "<u>Estimation par noyau (KDE)</u><br>\n",
    "<u>Fonction de répartition</u><br><br>\n",
    "<u>Fonction quantile</u><br>\n",
    "$\\mathbb{P}(X<q_{\\alpha})=\\alpha$<br><br>\n",
    "<u>Corrélation empirique</u><br>\n",
    "<u>Matrice de covariance</u><br>\n",
    "<font color='red'><u>Théorème spectral</u><br>\n",
    "Si $M$ est une matrice symétrique à coefficients réels, <br> alors il existe une matrice $U$ orthogonale et une matrice $D$ diagonale à coefficients réels tel que:<br> $M=UDU^{-1}$ <br></font>\n",
    "Si $M \\in \\mathbb{R}^{nxn}$,<br>\n",
    "$U=(U_1|...|U_n)$ vecteurs propres de $M$<br>\n",
    "$D=\\begin{pmatrix}\n",
    "\\lambda_1 & . & . \\\\\n",
    ". & \\ddots & . \\\\\n",
    ". & . & \\lambda_n\n",
    "\\end{pmatrix}$ valeurs propres de $M$<br>\n",
    "On peut retrouver le théorème avec la définition d'une valeur propre:<br>\n",
    "$\\lambda$ est valeur propre de $A$ s'il existe un vecteur propre $u$ non nul tel que $Mu=\\lambda u$<br>\n",
    "En utilisant $U$ et $D$ comme définis plus hauts,<br>\n",
    "$MU=UD$<br>\n",
    "$=>M=UDU^{-1}$<br>\n",
    "$=>M=UDU^T$<br>\n",
    "<font color='blue'>\n",
    "$U$ matrice orthogonale:<br>\n",
    "    *$U$ est inversible et $U^{-1}=U^T$<br>\n",
    "    *$U^TU=I$<br>\n",
    "    *Tous les vecteurs colonnes de $U$ sont orthogonaux et de norme 1. Ainsi, les colonnes de $U$ forment une base orthonormée.<br>\n",
    "</font>\n",
    "<u>Loi normale unidimensionnel</u><br>\n",
    "<font color='red'>Théorème Central Limite (TCL):\n",
    "\n",
    "$(X_n)_{n \\ge 1}$ une suite de variables réelles indépendantes et de même loi tel que $\\mu = \\mathbb{E}[X_1]$ et $\\mathbb{V}[X_1]=\\sigma^2$ sont définis ($\\mathbb{V}[X_1] \\leq +\\infty$)<br>\n",
    "En notant $\\bar{X}_n=\\frac{1}{n}(X_1 + ... + X_n)$ on  a:<br>\n",
    "$\\sqrt{n}\\frac{(\\bar{X}_n-\\mu)}{\\sigma} \\sim_{n \\to \\infty} \\mathcal{N(0,1)}$</font>\n",
    "\n",
    "<u>Vecteurs gaussiens</u><br>\n",
    "<u>Propriétés des vecteurs gaussiens</u><br>\n",
    "<u>Cholesky</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i>16/09</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle linéaire en dimension 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modélisation (équation)<br>\n",
    "Règle de Fermat / CNO<br>\n",
    "Expressions des coefficients<br>\n",
    "Centrage + mise à l'échelle<br>\n",
    "Prédicteur<br>\n",
    "Résidus<br>\n",
    "Vraisemblance<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle linéaire multivarié"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modélisation (équation)<br>\n",
    "Optimisation avec méthode du gradient<br>\n",
    "<u>Unicité</u><br>\n",
    "Il y a unicité de la solution lorsque $Ker(X)=0$ et $X$ est de plein rang<br><br>\n",
    "Démo:<br>\n",
    "* Noyau:<bR>\n",
    "    Si $\\widehat{\\theta_1}$, $\\widehat{\\theta_2}$ sont solutions du problème MCO:<br>\n",
    "    $X^TX\\widehat{\\theta_1}=X^TY$<br>\n",
    "    $X^TX\\widehat{\\theta_2}=X^TY$<br>\n",
    "    Ainsi,<br>\n",
    "    $X^TX(\\widehat{\\theta_1}-\\widehat{\\theta_2})=0$<br>\n",
    "    $\\widehat{\\theta_1}-\\widehat{\\theta_2} \\in Ker(X^TX)$<br>\n",
    "    Or <font color='orange'>$Ker(X^TX)=Ker(X)$</font><br>\n",
    "    Donc si $Ker(X)=0$ alors $\\widehat{\\theta_1} = \\widehat{\\theta_2}$ -> unicité\n",
    "* Rang:<br>\n",
    "    Quand il y a unicité, $Ker(X)=0$<br>\n",
    "    D'après le théorème du rang:<br>\n",
    "    $Dim(E)=Dim(Im(f))+Dim(Ker(f))$ où $f:E \\to F$<br>\n",
    "    $Dim(E)=rang(f)+Dim(Ker(f))$<br>\n",
    "    Dans notre cas, $f:\\mathbb{R}^p \\to \\mathbb{R}^n$ (car $X \\in \\mathbb{R}^{pxn}$ est la matrice associée à $f$), d'où<br>\n",
    "    $rang(f)=p-0=p$<br>\n",
    "    Il y a donc unicité quand la matrice X est de plein rang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i>23/09</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/Thm de projection<br>\n",
    "<font color = 'red'>Thm de projection (Hilbert): soit $\\mathcal{F} \\subset \\mathbb{R}^n$ et $y \\subset \\mathbb{R}^n$<br>\n",
    "Il est existe un unique point $z$ tel que: $\\inf_{z \\in \\mathcal{F}}||y-z||^2_2$<br>\n",
    "Ce point est caractérisé par les équations normales: <br>\n",
    "$<f,y-z>=0, \\forall f \\in \\mathcal{F}$</font>\n",
    "\n",
    "<font color='orange'>(schéma projection)</font>\n",
    "\n",
    "Application aux MCO<br>\n",
    "Dans le cas des MCO on a un unique point $\\inf||Y-X\\theta||_2^2$ caractérisé par les équations normales:<br>\n",
    "$<X,Y-X\\theta>=0$\n",
    "\n",
    "<font color='orange'>(schéma projection)</font>\n",
    "\n",
    "Thm unicité de l'estimateur<br><br>\n",
    "2/ Normalisation - centralisation des données<br><br>\n",
    "a) Modèle avec intercept<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$argmin \\|Y-1\\theta_0-\\tilde{X}\\theta\\|^2_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Modèle centré et sans intercept\n",
    "\n",
    "==> a) et b) équivalent, relation entre les estimateurs (formule)\n",
    "\n",
    "NORMALISATION : centrage + rescaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/ Etude théorique des MCO dans le cas du \"fixed design\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On suppose:\n",
    "$$\n",
    "Y=X\\theta^* +\\epsilon \\\\\n",
    "avec~\\epsilon \\in \\mathbb{R}^n \\\\\n",
    "X~déterministe \\\\\n",
    "\\epsilon_i~indépendantes,~centrées~(\\mathbb{E}[\\epsilon]=0) \\\\\n",
    "\\\\~\\\\\n",
    "\\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        cov(\\epsilon)=\\sigma^2I_n \\\\\n",
    "        \\mathbb{E}[\\epsilon]=0\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biais et Variance des coefficients:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Biais(\\widehat{\\theta_n}) = 0$ --> calculer $\\mathbb{E}[\\widehat{\\theta_n}-\\theta_n]$ et retrouver $\\mathbb{E}[\\epsilon]$\n",
    "\n",
    "$Var[\\widehat{\\theta_n}]=\\mathbb{V}[(X^TX)^{-1}X^T\\epsilon]=(X^TX)^{-1}X^T\\mathbb{V}[\\epsilon]((X^TX)^{-1}X^T)^T=\\sigma^2(X^TX)^{-1}$<br>\n",
    "Pour démontrer $\\mathbb{V}[AX]=A\\mathbb{V}[X]A^T$ on peut passer par la covariance (car $\\mathbb{V}[X]=Cov(X)=Cov(X,X)$) en montrant que $Cov(AX,AX)=ACov(X)A^T$ avec l'écriture sous forme d'espérances\n",
    "\n",
    "$R_{quad}[\\widehat{\\theta_n}]=Tr(Cov(\\widehat{\\theta_n}))+\\||Biais(\\widehat{\\theta_n})||_2^2\\$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas où le biais est nul:\n",
    "\n",
    "$R_{quad}[\\widehat{\\theta_n}]=Tr(Cov(\\widehat{\\theta_n}))=\\sigma^2(X^TX)^{-1}=\\frac{\\sigma^2}{n}Tr(\\widehat{G}^{-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> Problème de conditionnement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R_{pred}[\\widehat{\\theta_n}]=\\sigma^2*rang(H_X)$\n",
    "\n",
    "avec    $H_X=X(X^TX)^{-1}X^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i>30/09</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul / estimation de $\\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb{E}[\\sum{\\widehat{\\epsilon}^2}]=\\sigma^2(n-(p+1))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II/ Modèle sous-gaussien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On suppose:\n",
    "$$\n",
    "Y=X\\theta^* +\\epsilon \\\\\n",
    "avec~\\epsilon \\in \\mathbb{R}^n \\\\\n",
    "X~déterministe \\\\\n",
    "\\epsilon_i~indépendantes,~centrées,~\\textbf{sous-gaussiennes} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Une variable centrée Z est dit <b>sous-gaussienne</b> si:\n",
    "$$\\forall \\lambda \\in \\mathbb{R},~\\mathbb{E}[e^{\\lambda Z}]\\le e^{\\frac{\\lambda^2 \\sigma^2}{2}}$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Inégalité de Markov:\n",
    "soit X une VA positive ou nulle presque sûrement, alors:\n",
    "$$\\forall~a>0,~(|X|\\ge a)\\le\\frac{\\mathbb{E}[X]}{a}$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les inégalités de concentration fournissent des bornes aux VA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant Markov:\n",
    "\n",
    "<font color='red'>Inégalités de concentration:\n",
    "    \n",
    "$\\mathbb{P}(|S_n|>t)\\le 2e^{-\\frac{t^2}{2v_n}} \\\\\n",
    "avec~v_n=\\sum{\\sigma_i^2}$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$|\\widehat{\\theta}_{n,k}-\\theta^*_k|\\le \\sqrt{\\frac{2\\sigma^2log(2/\\sigma)}{n\\lambda_n}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III/ Modèle gaussien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On suppose:\n",
    "$$\n",
    "Y=X\\theta^* +\\epsilon \\\\\n",
    "X~déterministe \\\\\n",
    "\\epsilon_i~indépendantes,~centrées,~\\textbf{gaussiennes} \\\\\n",
    "\\mathbb{V}[\\epsilon_i]=\\sigma^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>Théorème de Cochran:\n",
    "soient $Q_i$ des formes quadratiques sur Y tel que $\\sum{Q_i}=\\sum Y_i^2$.\n",
    "\n",
    "Alors:\n",
    "\n",
    "$Q_j \\perp \\!\\!\\! \\perp ~~ et ~~ Q_j\\sim\\chi^2~~\\forall j$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du théorème de Cochran il en découle:\n",
    "\n",
    "(i) $\\widehat{\\theta}_{n,k} \\perp \\!\\!\\! \\perp \\widehat{\\sigma}^2_n~avec~\\widehat{\\sigma}^2_n=\\frac{1}{n-p-1}\\sum{}\\widehat{\\epsilon}^2_i$\n",
    "\n",
    "(ii) $\\sqrt{n}(\\widehat{\\theta}_n-\\theta^*)\\sim \\mathcal{N}(0,(X^TX)^{-1}\\sigma^2)$<br>\n",
    "Preuve:<br>\n",
    "$\\widehat{\\theta}_n=(X^TX)^{-1}X^TY=(X^TX)^{-1}X^T(X\\theta^{*}+\\epsilon)=\\theta^{*}+(X^TX)^{-1}X^T\\epsilon$<br>\n",
    "Or $\\epsilon \\sim \\mathcal{N}(0,\\sigma)$ car <b>modèle gaussien</b><br>\n",
    "Donc $\\theta^{*}+(X^TX)^{-1}X^T\\epsilon \\sim \\mathcal{N}(A,\\sqrt{B})$ où: <br>\n",
    "$A = \\mathbb{E}[\\theta^{*}+(X^TX)^{-1}X^T\\epsilon]=\\theta^{*}+(X^TX)^{-1}X^T\\mathbb{E}[\\epsilon]=\\theta^{*}$ et <br>\n",
    "$B = \\mathbb{V}[\\theta^{*}+(X^TX)^{-1}X^T\\epsilon]=(X^TX)^{-1}X^T\\mathbb{V}[\\epsilon]((X^TX)^{-1}X^T)^T=\\sigma^2(X^TX)^{-1}$<br>\n",
    "Donc $\\widehat{\\theta}_n \\sim \\mathcal{N}(\\theta^{*},\\sigma^2(X^TX)^{-1})$ <font color='orange'>(terme $\\sqrt{n}$??)</font><br>\n",
    "\n",
    "(iii) $\\widehat{\\sigma}^2_n \\frac{n-p-1}{\\sigma^2}\\sim \\chi^2(n-p-1)$\n",
    "\n",
    "(iv) $\\sqrt{\\frac{n}{\\widehat{\\sigma}^2 S_{n,k}}}(\\widehat{\\theta}_n-\\theta^*) \\sim T(n-p-1)~avec~S_{n,k}=e^T_k\\widehat{G}_ne_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i>07/10</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTERVALLES DE CONFIANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS PARAMETRIQUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Exemple 1: test de la moyenne d'une variable</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a $X_1,...,X_n~(iid)\\sim \\mathbb{P_\\theta}$ <br>\n",
    "\n",
    "$\n",
    "\\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        \\mathcal{H}_0: \\theta=a \\\\\n",
    "        \\mathcal{H}_1: \\theta=b \\\\\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$\n",
    "<br>\n",
    "(On suppose ici que $a<b$)\n",
    "\n",
    "Puisqu'on teste la moyenne, on choisit l'estimateur de la moyenne empirique: $\\widehat{\\theta}=\\frac{1}{n}\\sum{X_i}$ <br>\n",
    "On fixe $k$ pour un niveau de rejet $\\alpha$ et une zone de rejet:<br>\n",
    "$Z=\\{\\widehat{\\theta} \\ge k\\}$ (sous $\\mathcal{H}_1$, $\\widehat{\\theta}$ est grand i.e plus proche de $b$)\n",
    "\n",
    "On cherche $k$ défini tel que:<br>\n",
    "$\\mathbb{P_{\\theta\\ \\in \\Theta_0}}(\\widehat{\\theta} \\ge k)=\\alpha$ => sous $\\mathcal{H}_0$, on ne rejette l'hypothèse $\\mathcal{H}_0$ que lorsque notre estimateur $\\widehat{\\theta}$ est supérieur à $k$\n",
    "\n",
    "On centre et réduit pour retrouver la loi normale et trouver un intervalle à partir de quantiles que l'on connaît:\n",
    "\n",
    "$\\mathbb{P_{\\theta\\ = a}}(U \\ge \\frac{\\sqrt{n} (k-a)}{\\sqrt{\\sigma^2}})=\\alpha$ avec $U \\sim_{n \\to \\infty} \\mathcal{N}(0,1)$<br>\n",
    "$U$ est notre statistique de test (<font color='red'>une statistique de test est une variable aléatoire dont on connaît la loi sous $\\mathcal{H}_0$</font>)\n",
    "\n",
    "Ainsi, $\\frac{\\sqrt{n} (k-a)}{\\sqrt{\\sigma^2}}=q_\\alpha$ => on peut trouver $k$ qui nous dit quand rejeter $\\mathcal{H}_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Dans l'exemple ci-dessous, la valeur de $\\widehat{\\theta}$ ne nous permet pas de rejeter l'hypothèse $H_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Tests1.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Tests2.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i>14/10</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Exemple 2:  les coefficients sont-ils nuls? Illustration avec le modèle <u>forward selection</u></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a $X_1,...,X_n~(iid)\\sim \\mathbb{P_\\theta}$ <br>\n",
    "\n",
    "$\n",
    "\\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        \\mathcal{H}_0: \\theta_j=0~~~~~~~~~~~~~~~j \\in [\\![ 0,p ]\\!] \\\\\n",
    "        \\mathcal{H}_1: \\theta_j \\neq 0 \\\\\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$\n",
    "\n",
    "$\\widehat{\\theta}_j$ est ici notre estimateur.<br>\n",
    "$k_1$, $k_2$ fixés tel que $R=\\{\\theta_j<k_1\\}\\cup\\{\\theta_j>k_2\\}$<br>\n",
    "On veut que la probabilité de garder $\\mathcal{H}_0$ à raison soit maximale:<br>\n",
    "$\\mathbb{P_{\\theta\\ \\in \\Theta_0}}(k_1 < \\widehat{\\theta} < k_2)=1-\\alpha$<br>\n",
    "On veut maintenant transformer $\\theta_j$ pour avoir une loi dont on connaît les quantiles.<br>\n",
    "On sait que, <b>sous le modèle gaussien</b>, $\\frac{\\widehat{\\theta}_j-\\theta_j^*}{\\sigma} \\sim \\mathcal{N}(0,1)$<br>\n",
    "Cependant, on ne connaît pas $\\sigma$. On va essayer de retrouver une loi connue à l'aide la variance de $\\epsilon$.<br>\n",
    "$\\widehat{\\sigma}^2=\\frac{\\Sigma \\widehat{\\epsilon}_i^2}{n-rang(X)}$<br>\n",
    "Or <font color='red'>$\\Sigma_{i=1}^{n} Z_i^2 \\sim \\mathcal{X}^2(n)$ si $Zi \\sim_{iid} \\mathcal{N}(0,1)$ $\\forall i=\\{1,...,n\\}$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LE BOOTSTRAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Boostrap</i>: get oneself into a situation using <b>existing resources.</b><br>\n",
    "=> Méthode de réechantillonage utilisant le <b>tirage avec remise</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Principe du plug-in</font>: approcher une distribution inconnue par une <b>distribution empirique</b> évaluée sur un échantillon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La racine statistique est $\\hat{R}$ une fonction de $(X_1,...,X_n)$ qui converge en distribution vers $G$, i.e. $\\hat{R}\\sim G$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cherche donc la distribution de: $\\hat{R}=\\sqrt{n}(\\widehat{\\theta}-\\theta_{0})=\\sqrt{n}(T(\\mathbb{P_n})-T(\\mathbb{P}))$ où $T(\\mathbb{P_n})$ est une transformation de la distribution inconnue faisant intervenir la distribution empirique.<br>\n",
    "Rappel: $\\theta_0$ est la <b>vraie</b> quantité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'après le TCL, on a la convergence:<br>\n",
    "$$\\sqrt{n}(\\widehat{\\theta}-\\theta_{0})\\sim \\mathcal{N}(0,v)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Problèmes</b>:<br>\n",
    "On ne connaît pas le n optimal<br>\n",
    "La variance peut être dure à calculer<br>\n",
    "Sous le modèle gaussien, les ICs peuvent être biaisés<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Solution</b>:<br>\n",
    "On utilise le bootstrap pour simuler la loi de $\\sqrt{n}(T(\\mathbb{P_n})-T(\\mathbb{P}))$: <br><br>\n",
    "for b:1...B: <br>\n",
    "$(X_i)^* \\sim \\mathbb{P_n}$<br>\n",
    "Calcul de $T(\\mathbb{P_n^*})$<br>\n",
    "$R_b^*=\\sqrt{n}(T(\\mathbb{P_n^*})-T(\\mathbb{P_n}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(R_b^*)_{b=1...B}$ est l'échantillon Bootstrap qui simule la loi de $\\sqrt{n}(T(\\mathbb{P_n})-T(\\mathbb{P}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i>21/10</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rappels diagonalisation et SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rappel du <font color='red'>théorème spectral: $M=UDU^T$</font><br>\n",
    "En général on trie les valeurs propres par ordre décroissant: $\\lambda_1 \\ge \\lambda_2 \\ge ... \\ge \\lambda_p \\ge 0$<br>\n",
    "Dans le cas des MCO, on utilise cette décomposition pour étudier le spectre de la matrice de Gram ($\\frac{X^TX}{n}$). En effet, on a l'inversibilité si les valeurs propres sont strictement positives (voir démo dans le notebook de rappels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quand certaines valeurs propres sont nulles, on n'a pas l'unicité. Une solution est d'éliminer les valeurs propres nulles $\\lambda_{l+1} = ... = \\lambda_n = 0$:<br>\n",
    "$(X^TX)=UDU^T=\\Sigma_{i=1}^{n} \\lambda_i U_i U_i^T$<br>\n",
    "En sélectionnant uniquement les valeurs propres non nulles:<br>\n",
    "$(X^TX)=UDU^T=\\Sigma_{i=1}^{l} \\lambda_i U_i U_i^T$ et sa pseudo-inverse $(X^TX)^{+}= \\Sigma_{i=1}^{l} \\lambda_i^{-1} U_i U_i^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi utiliser la décomposition sur $X$ et non plus sur la matrice de Gram. Dans ce cas, on utilise la SVD pour la décomposition de matrice qui ne sont pas symétrique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>Singular Value Decomposition <br>\n",
    "C'est une généralisation de la diagonalisation pour n'importe quelle matrice.<br>\n",
    "SVD: soit $X \\in \\mathbb{R}^{nxp}$ à coefficients réels, alors $X=VSU^T$ avec:\n",
    "$\\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        V \\in \\mathbb{R}^{nxr} \\\\\n",
    "        S=diag(S_1,...,S_r) \\\\\n",
    "        U \\in \\mathbb{R}^{pxr}\n",
    "    \\end{array}\n",
    "\\right.$<br>\n",
    "et $U^TU=V^TV=I_r$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas des MCO, on a alors:<br>\n",
    "$\\widehat{\\theta_n}=(X^TX)^{+}X^TY \\\\\n",
    "=((VSU)^T(VSU))^{+}(VSU^T)^TY \\\\\n",
    "=((U^TS^TV^T)(VSU))^{+}(USV^T)Y \\\\\n",
    "=(U^TS^TSU)^{+}(USV^T)Y \\\\\n",
    "= (U^TS^2U)^{+}(USV^T)Y$<br> car $S$ matrice diagonale donc $S^TS=S^2$ (tous les coefficients sont au carré)<br>\n",
    "$=(\\Sigma \\lambda_i^{2}U_i U_i^T)^{+}(USV^T)Y \\\\\n",
    "=(\\Sigma \\lambda_i^{-2}U_i U_i^T)(USV^T)Y \\\\\n",
    "=(US^{-2}U^T)(USV^T)Y \\\\\n",
    "=(US^{-2}SV^T)Y \\\\\n",
    "=(US^{-1}V^T)Y \\\\\n",
    "=(\\Sigma \\lambda_i^{-1}U_i V_i^T)Y$<br>\n",
    "En posant $X^{+}=(US^{-1}V^T)$ on a $\\widehat{\\theta_n}=X^{+}Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problèmes de OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) Non-unicité de $\\widehat{\\theta_n}$: on peut toujours appliquer la méthode avec l'inverse généralisé $(X^TX)^{+}X^TY$<br>\n",
    "(ii) Valeurs propres très petites mais non nulles:<br>\n",
    "* L'estimateur $\\widehat{\\theta_n}=X^{+}Y$ devient très instable numériquement\n",
    "* La variance $Var[\\widehat{\\theta_n}]=\\sigma^2(X^TX)^{+}$ devient aussi instable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Solution 1: ACP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expression de la matrice de variance-covariance centrée $\\Sigma_n$\n",
    "\n",
    "Décomposition spectrale de $\\Sigma_n$\n",
    "\n",
    "Analyse des valeurs propres (spectre)\n",
    "\n",
    "On ne conserve que la dimension pour laquelle les valeurs propres sont les plus grandes --> ce sont celles qui expliquent le mieux la variance\n",
    "\n",
    "On réécrit la matrice de variance-covariance avec la dimension finale\n",
    "\n",
    "\"Comme les variables sont exprimées avec des unités différentes, on effectue l'ACP sur données centrées réduites, ce qui conduit à chercher les vaeurs et vecteurs propres de la matrice de corrélation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inconvénients: il peut y avoir de l'information perdue; dans les \"directions\" à faible variabilité peut résider une information essentielle à la prédiction de Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas MCO:<br>\n",
    "\n",
    "On veut réduire le nombre de variables à utiliser car sinon $X^TX$ est trop coûteux à inverser<br>\n",
    "Normalement, on travaille avec $X_c^TX_c$ car les variables explicatives sont (toujours?) centrées pour une ACP<br>\n",
    "Or $X_c^TX_c=n*Cov_n(X)$ où $Cov_n(X)$ est la covariance empirique de $X$<br>\n",
    "En effet, $Cov_n(X)=Cov_n(X,X)=\\frac{1}{n}\\Sigma_{i=1}^n(X_i-\\bar{X})(X_i-\\bar{X})^T=\\frac{X_c^TX_c}{n}$ où $\\bar{X}=\\frac{1}{n}\\Sigma_{i=1}^nX_i$<br>\n",
    "Donc réduire la dimension de $X_c^TX_c$ revient à réduire la dimension de $n*Cov_n(X)$<br>\n",
    "\n",
    "Pour réduire la dimension de $X_c^TX_c$, on décompose avec le théorème spectral $X_c^TX_c=UDU^T$<br>\n",
    "La matrice $D$ nous donne les valeurs propres de $X_c^TX_c$ (donc de $Cov_n(X))$<br>\n",
    "Les variables ayant les plus grandes valeurs propres sont celles qui expliquent le mieux la variance (<font color= 'orange'>pourquoi?</font>), on sélectionne donc les variables explicatives avec les $\\lambda_i$ les plus élevés)<br>\n",
    "Une fois les $k$ variables sélectionnées, on peut faire une régression sur $X\\tilde{U}$ où $\\tilde{U}=U_{[1:k]}$ car:<br>\n",
    "Soit $\\tilde{X}=X\\tilde{U}$<br>\n",
    "$\\tilde{X}^T\\tilde{X}=\\tilde{U}^TX^TX\\tilde{U}=\\tilde{U^T}UDU^T\\tilde{U}=\\tilde{D}$ où $\\tilde{D}=D_{[1:k]}$<br>\n",
    "Donc en travaillant avec $X\\tilde{U}$ on a une matrice de covariance $\\tilde{X}^T\\tilde{X}=\\tilde{D}$ (ici on ne considère pas le $n$) ou $\\tilde{D}$ est une matrice comportant les valeurs propres les plus importantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Solution 2: Forward selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Solution 3: Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée est d'ajouter une constante positive aux coefficients diagonaux pour permettre l'inversibilité sans difficulté numérique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{\\theta}^{Ridge}_{n,\\lambda} \\in argmin_{\\theta \\in \\mathbb{R}^{p+1}}~||Y-X\\theta||_2^2+\\lambda||\\theta||_2^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La minimisation donne $\\hat{\\theta}^{Ridge}_{n,\\lambda}=(X^TX+\\lambda Id_n)^{-1}X^TY$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Démo:<br>\n",
    "\n",
    "$f: \\theta \\mapsto \\frac{1}{2}||Y-X\\theta||_2^2+\\frac{\\lambda}{2}||\\theta||_2^2$\n",
    "\n",
    "$f(\\theta)=\\frac{1}{2}\\Sigma_{i=1}^n(Y_i-X_i\\theta)^2+\\frac{\\lambda}{2} \\Sigma_{i=1}^n \\theta_i^2$ (écriture sans les normes)\n",
    "\n",
    "$\\nabla f(\\theta) = 0 => \\forall k=1,...,p: \\frac{\\partial f(\\theta)}{\\partial \\theta_k}=0$\n",
    "\n",
    "$\\frac{\\partial f(\\theta)}{\\partial \\theta_k}=2\\frac{1}{2} (-X_{i,k}) \\Sigma_{i=1}^n (Y_i-\\Sigma_{j=1}^pX_{i,j}\\theta_j)+2\\frac{\\lambda}{2} \\theta_k$\n",
    "\n",
    "$~~~~~~~=-\\Sigma_{i=1}^n(X_{i,k}Y_i-X_{i,k}X_i\\theta)+\\lambda \\theta_k$\n",
    "\n",
    "En généralisant sur toutes les composantes, $\\nabla f(\\theta)=-\\Sigma_{i=1}^n(X_i^TY_i-X_i^TX_i\\theta)+\\lambda \\theta=-X^TY+X^TX\\theta + \\lambda \\theta$\n",
    "\n",
    "Ainsi $\\nabla f(\\theta)=0 => -X^TY+X^TX\\theta + \\lambda \\theta=0$\n",
    "$=> \\theta(X^TX + \\lambda Id_n) = X^TY => \\theta = (X^TX + \\lambda Id_n)^{-1} X^TY = \\hat{\\theta}_{n,\\lambda}^{Ridge} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:<br>\n",
    "$\\mathbb{E}[\\hat{\\theta}_{n,\\lambda}^{Ridge}]=$<br>\n",
    "$\\mathbb{V}[\\hat{\\theta}_{n,\\lambda}^{Ridge}]=$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choix du $\\lambda$<br>\n",
    "\n",
    "<i>Validation croisée</i>: méthode d’estimation de fiabilité d’un modèle fondé sur une technique d’échantillonnage\n",
    "\n",
    "On divise l'échantillon en $k$ <i>fold</i><br>\n",
    "Pour tout $k=1 ... K$<br>\n",
    "- On calcule l'estimateur de Ridge sur tous les échantillons sauf le k-ième\n",
    "- On calule son erreur (somme des erreurs au carré)\n",
    "Pour trouver le $\\lambda$ optimal, on minimise la somme des mesure des erreurs des estimateurs calculés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i>22/10</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Méthode de pénalisation\n",
    "\n",
    "Différences avec Ridge:\n",
    "\n",
    "\n",
    "- Comme Ridge sauf qu'on utilise la norme 1\n",
    "- A la différence de Ridge ici les variables sont sélectionnées\n",
    "- Hypothèse des valeurs propres sur $X^TX$ est changée en une hypothèse ne concernant que les éléments du cône"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
