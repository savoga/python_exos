{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation-Maximziation (EM) in the case of Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GMM sample is composed of $j$ Gaussian variables (<i>clusters</i>) distributed with proportions $(\\pi_1,...,\\pi_k)$ ($\\Sigma \\pi_i =1$)\n",
    "\n",
    "One can write:\n",
    "\n",
    "$$X \\sim \\mathcal{N}(\\mu_{Z},\\Sigma_{Z})~~~~~~with~~Z \\sim \\pi$$\n",
    "\n",
    "$\\pi$ is not really a law but more the proportions of each Gaussian categories.\n",
    "\n",
    "Thus, $X$ has a density (weighted-average of all Gaussian densities):\n",
    "\n",
    "$$p_\\theta(x) = \\Sigma_{j=1}^{k}\\pi_j f_j(x)~~~~~~~(*)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to estimate $\\theta = (\\pi, \\mu, \\Sigma)$ where:\n",
    "\n",
    "$\\pi=(\\pi_1,...,\\pi_k)$\n",
    "\n",
    "$\\mu=(\\mu_1,...,\\mu_k)$\n",
    "\n",
    "$\\Sigma=(\\Sigma_1,...,\\Sigma_k)$\n",
    "\n",
    "To do so, we use the maximum likelihood method (product of densities across all samples):\n",
    "\n",
    "$$p_\\theta(x)=\\Pi_{i=1}^n p_\\theta(x_i)$$\n",
    "\n",
    "$$l(\\theta)=log(\\Pi_{i=1}^n p_\\theta(x_i))=\\Sigma_{i=1}^n log(p_\\theta(x_i))$$\n",
    "\n",
    "We thus need to find $argmax(l(\\theta))$\n",
    "\n",
    "Problem: the likelihood function is not convex!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectation-maximization problem is used when we have <i>latent variables</i> (= variables for which we don't know their associated distribution).\n",
    "\n",
    "Let $z=(z_1,...,z_k)$ be the vector of latent variables. We can express the density $(*)$ as a joint function:\n",
    "\n",
    "$$p_\\theta(x,z)=p_\\theta(z)p_\\theta(x|z)$$\n",
    "\n",
    "$$l(\\theta, z) = ... =\\Sigma(log \\pi_{z_i})+ \\Sigma(logf_{z_i}(x_i))$$\n",
    "\n",
    "A classic optimization (in case of Gaussians) give us empirical values as solutions e.g. $\\hat{\\pi_j}=\\frac{n_j}{n}$\n",
    "\n",
    "Problem: we don't know $j$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will thus use the <i>expected</i> log-likelihood method.\n",
    "\n",
    "Let us find another expression of the likelihood:\n",
    "\n",
    "$$p_\\theta(x,z)=p_\\theta(x)p_\\theta(z|x)$$\n",
    "\n",
    "As seen previously: $p_\\theta(x,z)=\\Pi \\pi_{z_i}f_{z_i}(x_i)$\n",
    "\n",
    "$p\\theta(z|x)=\\Pi p_\\theta(z_i|x_i)=\\frac{\\Pi \\pi_{z_i}f{z_i}(x_i)}{p_\\theta(x_i)} \\propto \\Pi \\pi_{z_i}f{z_i}(x_i)$\n",
    "\n",
    "Given an initial parameter $\\theta_0$ The <i>expected</i> log-likelihood is written as such:\n",
    "\n",
    "$$\\mathbb{E}_{\\theta_0}[l(\\theta;z)]=\\Sigma p_{\\theta_0}(z|x) l(\\theta;z)$$\n",
    "$$\\mathbb{E}_{\\theta_0}[l(\\theta;z)]=\\Sigma_{j} \\Sigma_{i} p_{ij}(log\\pi_j+logf_j(x_i))$$\n",
    "\n",
    "We now have an expression that doesn't depend on $z$ but only on $p_{ij}$ and we know that $n_j=\\Sigma_i p_{ij}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
